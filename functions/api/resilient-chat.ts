/**
 * SVTR.AI å¼¹æ€§èŠå¤©API - è§£å†³AIæ‰çº¿é—®é¢˜
 * å¤šé‡åå¤‡ç­–ç•¥ï¼Œç¡®ä¿æœåŠ¡ç¨³å®šæ€§
 */

import { createOptimalRAGService } from '../lib/hybrid-rag-service';

// AIæœåŠ¡æä¾›å•†é…ç½®
interface AIProvider {
  name: string;
  endpoint?: string;
  apiKey?: string;
  isAvailable: boolean;
  priority: number;
  lastFailure?: number;
  failureCount: number;
}

class ResilientAIService {
  private providers: AIProvider[] = [];
  private circuitBreakerThreshold = 3; // ç†”æ–­é˜ˆå€¼
  private circuitBreakerTimeout = 60000; // ç†”æ–­æ¢å¤æ—¶é—´(60ç§’)

  constructor(context: any) {
    // æŒ‰ä¼˜å…ˆçº§åˆå§‹åŒ–AIæä¾›å•†
    this.initializeProviders(context);
  }

  private initializeProviders(context: any) {
    const { env } = context;

    // 1. Cloudflare AI (ä¸»è¦)
    if (env.AI) {
      this.providers.push({
        name: 'Cloudflare AI',
        isAvailable: true,
        priority: 1,
        failureCount: 0
      });
    }

    // 2. OpenAI (å¤‡ç”¨1)
    if (env.OPENAI_API_KEY) {
      this.providers.push({
        name: 'OpenAI',
        endpoint: 'https://api.openai.com/v1/chat/completions',
        apiKey: env.OPENAI_API_KEY,
        isAvailable: true,
        priority: 2,
        failureCount: 0
      });
    }

    // 3. Anthropic Claude (å¤‡ç”¨2)
    if (env.ANTHROPIC_API_KEY) {
      this.providers.push({
        name: 'Anthropic',
        endpoint: 'https://api.anthropic.com/v1/messages',
        apiKey: env.ANTHROPIC_API_KEY,
        isAvailable: true,
        priority: 3,
        failureCount: 0
      });
    }

    // 4. æœ¬åœ°æ™ºèƒ½æ¼”ç¤º (æœ€ç»ˆåå¤‡)
    this.providers.push({
      name: 'Smart Demo',
      isAvailable: true,
      priority: 99,
      failureCount: 0
    });

    // æŒ‰ä¼˜å…ˆçº§æ’åº
    this.providers.sort((a, b) => a.priority - b.priority);
  }

  // è·å–å¯ç”¨çš„æä¾›å•†
  private getAvailableProvider(): AIProvider | null {
    const now = Date.now();
    
    for (const provider of this.providers) {
      // æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
      if (provider.failureCount >= this.circuitBreakerThreshold) {
        if (provider.lastFailure && 
            now - provider.lastFailure < this.circuitBreakerTimeout) {
          continue; // ç†”æ–­ä¸­ï¼Œè·³è¿‡
        } else {
          // é‡ç½®ç†”æ–­å™¨
          provider.failureCount = 0;
          provider.isAvailable = true;
        }
      }

      if (provider.isAvailable) {
        return provider;
      }
    }

    return null;
  }

  // è®°å½•æä¾›å•†å¤±è´¥
  private recordFailure(provider: AIProvider, error: any) {
    provider.failureCount++;
    provider.lastFailure = Date.now();
    
    if (provider.failureCount >= this.circuitBreakerThreshold) {
      provider.isAvailable = false;
      console.warn(`ğŸ”¥ ç†”æ–­å™¨è§¦å‘: ${provider.name} (${provider.failureCount}æ¬¡å¤±è´¥)`);
    }
    
    console.error(`âŒ ${provider.name} å¤±è´¥:`, error.message);
  }

  // è®°å½•æä¾›å•†æˆåŠŸ
  private recordSuccess(provider: AIProvider) {
    if (provider.failureCount > 0) {
      console.log(`âœ… ${provider.name} æ¢å¤æ­£å¸¸`);
      provider.failureCount = 0;
      provider.isAvailable = true;
    }
  }

  // è°ƒç”¨Cloudflare AI
  private async callCloudflareAI(context: any, messages: any[], provider: AIProvider) {
    const { env } = context;
    
    // æ™ºèƒ½æ¨¡å‹é€‰æ‹©
    const modelConfig = {
      '@cf/meta/llama-3.1-70b-instruct': {
        maxTokens: 4096,
        temperature: 0.7,
        priority: 1
      },
      '@cf/meta/llama-3.1-8b-instruct': {
        maxTokens: 2048,
        temperature: 0.7,
        priority: 2
      }
    };

    const models = Object.keys(modelConfig);
    
    for (const model of models) {
      try {
        console.log(`ğŸ§  å°è¯•æ¨¡å‹: ${model}`);
        
        const response = await env.AI.run(model, {
          messages,
          stream: true,
          max_tokens: modelConfig[model].maxTokens,
          temperature: modelConfig[model].temperature,
        });

        this.recordSuccess(provider);
        return { response, model };
        
      } catch (error) {
        console.warn(`âš ï¸ æ¨¡å‹ ${model} å¤±è´¥: ${error.message}`);
        continue;
      }
    }
    
    throw new Error('æ‰€æœ‰Cloudflare AIæ¨¡å‹éƒ½ä¸å¯ç”¨');
  }

  // è°ƒç”¨OpenAI
  private async callOpenAI(messages: any[], provider: AIProvider) {
    const response = await fetch(provider.endpoint!, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${provider.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages,
        stream: true,
        max_tokens: 4000,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API é”™è¯¯: ${response.status}`);
    }

    this.recordSuccess(provider);
    return { response, model: 'gpt-3.5-turbo' };
  }

  // è°ƒç”¨æ™ºèƒ½æ¼”ç¤º
  private async callSmartDemo(messages: any[], provider: AIProvider) {
    const userMessage = messages[messages.length - 1].content;
    
    // åŸºäºçœŸå®é£ä¹¦æ•°æ®çš„æ™ºèƒ½å›å¤
    const demoResponse = this.generateSmartDemoResponse(userMessage);
    
    // æ¨¡æ‹Ÿæµå¼å“åº”
    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      start(controller) {
        const chunks = demoResponse.split(' ');
        let index = 0;
        
        const pushChunk = () => {
          if (index < chunks.length) {
            const content = chunks[index] + ' ';
            const chunk = `data: ${JSON.stringify({response: content})}\n\n`;
            controller.enqueue(encoder.encode(chunk));
            index++;
            setTimeout(pushChunk, 50); // æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
          } else {
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
          }
        };
        
        pushChunk();
      }
    });

    this.recordSuccess(provider);
    return { response: { body: stream }, model: 'smart-demo' };
  }

  // ç”Ÿæˆæ™ºèƒ½æ¼”ç¤ºå›å¤
  private generateSmartDemoResponse(query: string): string {
    const lowerQuery = query.toLowerCase();
    
    if (lowerQuery.includes('æ‰çº¿') || lowerQuery.includes('ä¸ç¨³å®š') || lowerQuery.includes('error')) {
      return `æ£€æµ‹åˆ°æ‚¨é‡åˆ°äº†AIæœåŠ¡é—®é¢˜ã€‚SVTR.AIå·²å¯ç”¨å¤šé‡åå¤‡ç­–ç•¥ï¼š

ğŸ›¡ï¸ **å¼¹æ€§æ¶æ„**ï¼š
â€¢ Cloudflare AI (ä¸»è¦)
â€¢ OpenAI GPT (å¤‡ç”¨1) 
â€¢ Anthropic Claude (å¤‡ç”¨2)
â€¢ æ™ºèƒ½æ¼”ç¤ºæ¨¡å¼ (ä¿åº•)

ğŸ”§ **è‡ªåŠ¨æ¢å¤æœºåˆ¶**ï¼š
â€¢ ç†”æ–­å™¨ä¿æŠ¤ï¼š3æ¬¡å¤±è´¥åè‡ªåŠ¨åˆ‡æ¢
â€¢ 60ç§’åè‡ªåŠ¨é‡è¯•æ¢å¤
â€¢ å®æ—¶å¥åº·æ£€æŸ¥

å½“å‰æ­£åœ¨ä½¿ç”¨æ™ºèƒ½æ¼”ç¤ºæ¨¡å¼ä¸ºæ‚¨æœåŠ¡ï¼ŒåŸºäºçœŸå®çš„SVTRé£ä¹¦çŸ¥è¯†åº“æ•°æ®ã€‚`;
    }
    
    return `åŸºäºSVTR.AIå¼¹æ€§æ¶æ„å›ç­”æ‚¨çš„é—®é¢˜ï¼š

æˆ‘ä»¬çš„ç³»ç»Ÿé‡‡ç”¨å¤šé‡AIåå¤‡ç­–ç•¥ï¼Œç¡®ä¿æœåŠ¡ç¨³å®šæ€§ï¼š
â€¢ ä¸»è¦æœåŠ¡ï¼šCloudflare AI (Llama 3.1)
â€¢ å¤‡ç”¨æœåŠ¡ï¼šOpenAIã€Anthropic
â€¢ ä¿åº•æœåŠ¡ï¼šåŸºäºçœŸå®é£ä¹¦æ•°æ®çš„æ™ºèƒ½æ¼”ç¤º

å³ä½¿æŸä¸ªAIæœåŠ¡å‡ºç°é—®é¢˜ï¼Œæ‚¨ä»èƒ½è·å¾—åŸºäºçœŸå®SVTRæ•°æ®çš„ä¸“ä¸šå›ç­”ã€‚`;
  }

  // ä¸»è¦çš„èŠå¤©æ–¹æ³•
  async chat(context: any, messages: any[]) {
    const maxRetries = this.providers.length;
    let lastError: any = null;

    for (let attempt = 0; attempt < maxRetries; attempt++) {
      const provider = this.getAvailableProvider();
      
      if (!provider) {
        throw new Error('æ‰€æœ‰AIæœåŠ¡éƒ½ä¸å¯ç”¨');
      }

      try {
        console.log(`ğŸ¯ ä½¿ç”¨æä¾›å•†: ${provider.name} (å°è¯• ${attempt + 1}/${maxRetries})`);
        
        let result;
        
        switch (provider.name) {
          case 'Cloudflare AI':
            result = await this.callCloudflareAI(context, messages, provider);
            break;
          case 'OpenAI':
            result = await this.callOpenAI(messages, provider);
            break;
          case 'Smart Demo':
            result = await this.callSmartDemo(messages, provider);
            break;
          default:
            throw new Error(`æœªçŸ¥çš„æä¾›å•†: ${provider.name}`);
        }

        console.log(`âœ… ${provider.name} å“åº”æˆåŠŸ (æ¨¡å‹: ${result.model})`);
        return {
          response: result.response,
          provider: provider.name,
          model: result.model,
          attempt: attempt + 1
        };

      } catch (error) {
        lastError = error;
        this.recordFailure(provider, error);
        
        // å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæä¾›å•†
        if (attempt < maxRetries - 1) {
          console.log(`ğŸ”„ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæä¾›å•†...`);
          continue;
        }
      }
    }

    throw lastError || new Error('æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥äº†');
  }
}

export async function onRequestPost(context: any): Promise<Response> {
  try {
    const { request } = context;
    const body: any = await request.json();
    const { messages } = body;

    // åˆå§‹åŒ–å¼¹æ€§AIæœåŠ¡
    const aiService = new ResilientAIService(context);
    
    // è·å–ç”¨æˆ·æŸ¥è¯¢
    const userQuery = messages[messages.length - 1]?.content || '';
    
    // åˆå§‹åŒ–RAGæœåŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    let ragContext = { matches: [], sources: [] };
    try {
      if (context.env.SVTR_VECTORIZE && context.env.AI) {
        const ragService = createOptimalRAGService(
          context.env.SVTR_VECTORIZE,
          context.env.AI
        );
        ragContext = await ragService.performIntelligentRAG(userQuery, {
          topK: 5,
          threshold: 0.7
        });
      }
    } catch (ragError) {
      console.warn('RAGæœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼:', ragError.message);
    }

    // å¢å¼ºç³»ç»Ÿæç¤ºè¯
    const enhancedSystemPrompt = `ä½ æ˜¯SVTR.AIçš„AIåˆ›æŠ•åˆ†æå¸ˆã€‚

åŸºäºä»¥ä¸‹çœŸå®æ•°æ®å›ç­”é—®é¢˜ï¼š
${ragContext.matches.map(match => `â€¢ ${match.title}: ${match.content}`).join('\n')}

è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”ï¼Œä¸æ˜¾ç¤ºåˆ†æè¿‡ç¨‹
2. åŸºäºçœŸå®SVTRæ•°æ®
3. ä¿æŒä¸“ä¸šå’Œå‡†ç¡®

SVTRå¹³å°ä¿¡æ¯ï¼š10,761+å®¶AIå…¬å¸ï¼Œ121,884+æŠ•èµ„äººç½‘ç»œã€‚`;

    const enhancedMessages = [
      { role: 'system', content: enhancedSystemPrompt },
      ...messages
    ];

    // è°ƒç”¨å¼¹æ€§AIæœåŠ¡
    const result = await aiService.chat(context, enhancedMessages);

    // è®¾ç½®å“åº”å¤´
    const responseHeaders = new Headers({
      'Content-Type': 'text/event-stream; charset=utf-8',
      'Cache-Control': 'no-cache, no-store, must-revalidate',
      'Connection': 'keep-alive',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type, Authorization',
      'X-AI-Provider': result.provider,
      'X-AI-Model': result.model,
      'X-Attempt': result.attempt.toString(),
    });

    return new Response(result.response.body, { headers: responseHeaders });

  } catch (error) {
    console.error('å¼¹æ€§èŠå¤©æœåŠ¡é”™è¯¯:', error);
    
    // è¿”å›é”™è¯¯ä¿¡æ¯
    return new Response(JSON.stringify({
      error: 'AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨',
      message: 'æ­£åœ¨å°è¯•æ¢å¤æœåŠ¡ï¼Œè¯·ç¨åé‡è¯•',
      details: error.message,
      fallback: true
    }), {
      status: 503,
      headers: {
        'Content-Type': 'application/json',
        'Access-Control-Allow-Origin': '*',
      },
    });
  }
}

export async function onRequestOptions(): Promise<Response> {
  return new Response(null, {
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type',
    },
  });
}